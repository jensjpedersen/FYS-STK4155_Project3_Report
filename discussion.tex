%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Manual feature selection}

% sklearn random forest  

\begin{table}
    \centering
    \caption{Ranking of feature importance from best to worst with respect to
    impurity descrease for three different reconstruction methods on the full
    dataset. }  
    \label{tab:feature_importance}  

    \begin{tabular}{|c|c|c|c|}
        \hline
        Rank & Feature index & Feature name & Impurity decrease \\     
        \hline
        0       & 2           & histogram min  & 0.142238   \\
        1      & 11           & glcm contrast  & 0.112067   \\
        2      & 12            & glcm entropy  & 0.110245   \\
        3      & 13        & glcm homogeneity  & 0.107881   \\
        4       & 4           & histogram std  & 0.106380   \\
        5      & 14      & glcm joint\_maximum  & 0.081486   \\
        6       & 0           & histogram max  & 0.054979   \\
        7       & 5      & shape area\_density  & 0.051267   \\
        8       & 3          & histogram peak  & 0.046761   \\
        9       & 8            & shape volume  & 0.043067   \\
        10      & 1          & histogram mean  & 0.037393   \\
        11     & 10            & glcm cluster  & 0.033017   \\
        12      & 7           & shape surface  & 0.027778   \\
        13      & 9    & shape volume\_density  & 0.023328   \\
        14      & 6  & shape convex\_hull\_area  & 0.022114   \\
        \hline
         
    \end{tabular} 
\end{table}


% Identify features with poor class separation

% Identify highly correlated features  
% * plot correlation matrix  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/feature_importance.png}
    \caption{Mean of the feautre\_importacnes\_ (purity decrease) for each decision tree. The
    error bar shows the 95\% confidence interval. The x-axis represent feature
    number as defined in table XXX: ref table.  }  
    \label{fig:feature_importance} 
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Figures/feature_correlation.png}
    \caption{Pearson Correlation between each feature calculated on the full
    dataset. }  
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compare Manual VS Automatic feature reduction}
\begin{itemize}
    \item Is the Automatic selection method as expected, compared with visual
        inspection, manual feature reduction.   
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SVM}

\subsubsection{Classification of class 1 vs class 4}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)0}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-0}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs0}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^5$  $\gamma=1 $ respectively. }
\label{fig:Figures-feature_pairs0}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature0}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^5$  $\gamma=1 $ respectively. }
\label{fig:Figures-third_feature0}
\end{figure}

\subsubsection{Classification of class 0 vs class 4}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)1}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-1}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs1}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=100$  $\gamma=10 $ respectively. }
\label{fig:Figures-feature_pairs1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature1}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=100$  $\gamma=10 $ respectively. }
\label{fig:Figures-third_feature1}
\end{figure}


\subsubsection{Classification of class 0 vs class 1}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)4}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-4}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs4}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^4$  $\gamma=1 $ respectively. }
\label{fig:Figures-feature_pairs4}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature4}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^4$  $\gamma=1 $ respectively. }
\label{fig:Figures-third_feature4}
\end{figure}



