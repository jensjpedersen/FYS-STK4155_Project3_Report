%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Manual feature selection}

% sklearn random forest  

\begin{table}[H]
    \centering
    \caption{Ranking of feature importance from best to worst with respect to
    impurity decrease for three different reconstruction methods on the full
    dataset obtained with sci-kit learn.}  
    \label{tab:feature_importance}  

    \begin{tabular}{|c|c|c|c|}
        \hline
        Rank & Feature index & Feature name & Impurity decrease \\     
        \hline
        0       & 2           & histogram min  & 0.102915  \\
        1      & 12            & glcm entropy  & 0.088981  \\
        2      & 11           & glcm contrast  & 0.083777  \\
        3       & 4           & histogram std  & 0.081181  \\
        4      & 13        & glcm homogeneity  & 0.080463  \\
        5       & 5      & shape area\_density  & 0.073690  \\
        6       & 3          & histogram peak  & 0.069483  \\
        7       & 0           & histogram max  & 0.064857  \\
        8      & 14      & glcm joint\_maximum  & 0.061548  \\
        9       & 9    & shape volume\_density  & 0.055089  \\
        10      & 8            & shape volume  & 0.051088  \\
        11      & 7           & shape surface  & 0.048897  \\
        12     & 10            & glcm cluster  & 0.048494  \\
        13      & 1          & histogram mean  & 0.047758  \\
        14      & 6  & shape convex\_hull\_area  & 0.041778  \\
        \hline
         
    \end{tabular} 
\end{table}


% Identify features with poor class separation

% Identify highly correlated features  
% * plot correlation matrix  

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/feature_importance_sklearn_3s.png}
    \caption{Mean of the feautre\_importacnes\_ (purity decrease) for each decision tree. The
    error bar shows the 95\% confidence interval. The x-axis represent feature
    number as defined in table \ref{tab:feature_names}.  }  
    \label{fig:feature_importance} 
\end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{Figures/feature_importance.png}
%     \caption{Mean of the feautre\_importacnes\_ (purity decrease) for each decision tree. The
%     error bar shows the 95\% confidence interval. The x-axis represent feature
%     number as defined in table XXX: ref table.  }  
%     \label{fig:feature_importance} 
% \end{figure}

Figure \ref{fig:feature_importance} shows the histogram the feature importance
on the y axis and feature index on the x axis. These results was obtained with
with sci-kit learn's implementation of the random forest. The 95 \% confidence
interval is shown on the top of each bar. The histogram min is best able to
discriminate between the tree different reconstruction methods, with a impurity
decrease of 0.1029\&. The numerical feature importance (Impurity decrease)
values for all the features is listed in table \ref{tab:feature_importance}.
The glcm entropy features gives the second best impurity decrease with a value
of 0.0890. Our next 3 best features are glcm contrast, histogram std and glcm
homogeneity which performs similarity with an impurity descrese in the rande
0.0805-0.0838. 

In figure \ref{fig:correlation} the correlation between all the features is
plotted. The color bar has a range from -1 (black) to 1 (white), where a score
of 1 indicates perfect correlation. The histogram min features is not strongly
correlated with any other features, thus we expect this features give a good
accuracy on the test data with the SVM model. Our next best features glcm
entorpy is highly correlated with histogram std. Thus, only one of the features
should be used when training a classification model. Utilizing both features
will produce high variance due to redundant information. When training our SVM
we therefore expect the combinations of these two features to produce worse
accuracy on the test data compared with any other combination of the top 5
features. Glcm contrast has the highest correlation with histogram std, but not
to the same extent as the two previous discussed features. GLCM homogeneity has
the highest correlation with glcm joint\_maximum. However, the GLCM maximum
features performance significantly worse with a difference impurity decrease of
0.0189. In our training of the SVM we expect that a combination of hisogram
min, glcm entropy. The 4 combinations of features we expect to
produce the best predication on the test data is listed in table
\ref{tab:expectation}. Also, every combination of two features in each row is
expected perform well with SVM. 

\begin{table}
    \centering
    \caption{Each row corresponds to a set of feature combinations that is
        expected to give good accuracy with SVM. Combinations of two of the features
    in each row shoud also give good accuracy with SVM. The columns is sorted
with respect to impurity decrease, where the left most column has the highest
impurity decrease}  
    \label{tab:expectation} 
    \begin{tabular}{|c|c|c|}
        \hline
        feature 1 & feature 2 & feature 3 \\
        \hline
        histogram min & gclm entropy & glcm homogeneity \\ 
        histogram min & gclm entropy & histogram std  \\ 
        histogram min & gclm contrast & glcm homogeneity \\ 
        histogram min & gclm contrast & histogram std  \\ 
        \hline
    \end{tabular} 
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Figures/feature_correlation.png}
    \caption{Pearson Correlation between each feature calculated on the full
    dataset. }  
    \label{fig:correlation} 
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Compare Manual VS Automatic feature reduction}
\begin{itemize}
    \item Is the Automatic selection method as expected, compared with visual
        inspection, manual feature reduction.   
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SVM}

\subsubsection{Classification of class 1 vs class 4}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)0}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-0}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs0}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^5$  $\gamma=1 $ respectively. }
\label{fig:Figures-feature_pairs0}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature0}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^5$  $\gamma=1 $ respectively. }
\label{fig:Figures-third_feature0}
\end{figure}

\subsubsection{Classification of class 0 vs class 4}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)1}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-1}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs1}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=100$  $\gamma=10 $ respectively. }
\label{fig:Figures-feature_pairs1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature1}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=100$  $\gamma=10 $ respectively. }
\label{fig:Figures-third_feature1}
\end{figure}


\subsubsection{Classification of class 0 vs class 1}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/accuracy(C,gamma)4}
\caption{Heatmap of the accuracy obtained with different slack constants $C$ and 
GRBF kernel factors $\gamma $ in the SVM model training with all the features. The SVM accuracies are the average of $30$ cross-validation 
cycles training and predicting test data of relative size $0.25$.}
\label{fig:Figures-accuracy-C-gamma-4}
\end{figure}

\begin{figure}[H]
\centering
\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{Figures/feature_pairs4}}
\caption{Heatmap of the accuracy obtained by the SVM model training on different feature pairs. The SVM accuracies are the average 
of $20$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^4$  $\gamma=1 $ respectively. }
\label{fig:Figures-feature_pairs4}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Figures/third_feature4}
\caption{Bar plot of the accuracy obtained adding a third feature in the data used 
by the SVM model. The accuracies are the average 
of $50$ cross-validation cycles training and predicting test data of relative size $0.25$.
The slack constant and GRBF kernel factor are set to $C=10^4$  $\gamma=1 $ respectively. }
\label{fig:Figures-third_feature4}
\end{figure}



\autoref{fig:Figures-accuracy-C-gamma-0} shows the accuracy of the SVM model's 
test data predictions for different slack constants $C$ and GRBF kernel factors $\gamma $. 
We observe the best accuracy for $C=10^5$ and $\gamma =1$. Lower slack constant 
allows for more misclassification while lower $\gamma $ results in a wider 
GRBF kernel which means that the influence of each data point on the hyperplane 
position will decrease. Both leads towards under fitting on the under-over fit spectrum.
Our optimal values for $C$ and $\gamma $ are quite large suggesting the need for 
a complex hyperplane to classify our data. 

We will utilize these optimal value in our analysis of predictions based on two and three 
features. They should be tuned for every feature combination, but this would required 
way to much computation to handle for our computer. The optimal parameter values 
obtained using all the features will be good general parameters for the following 
feature combination analysis. 


\autoref{fig:Figures-feature_pairs0} shows a heatmap of the accuracy obtained with different 
feature pairs utilized by the SVM model for training and predicting. The diagonal from upper left 
to lower right of the heatmap shows pairs of same features which is equivalent to classification 
based on that single feature. We observe the best score for a single feature with \verb|glcm entropy|.

The feature pairs have been utilized twice, one on either side of the equal feature diagonal, which is nice as 
this gives some conformation. We observe both scores of the feature pair \verb|histogram min| and \verb|glcm homogeneity| 
to be the best. 

