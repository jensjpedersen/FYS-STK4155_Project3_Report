%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
\paragraph{Resources Hyperlinks} 
\begin{itemize}
    \item \href{https://www.kaggle.com/code/rafjaa/dealing-with-very-small-datasets}{Dealing with very small datasets | Gaggle}
    
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset}

\begin{table}
    \centering
    \caption{List of all features used in analysis. The exact feature
    definitions can be found in the appendix \ref{sec:appendix_feature_definitions}}  
    \label{tab:features} 
    \begin{tabular}{|c|c|}
        \hline
        Feature number & Feature name \\
        \hline
        0            & histogram max \\
        \hline
        1           & histogram mean \\
        \hline
        2            & histogram min \\
        \hline
        3           & histogram peak \\
        \hline
        4            & histogram std \\
        \hline
        5       & shape area\_density \\
        \hline
        6   & shape convex\_hull\_area \\
        \hline
        7            & shape surface \\
        \hline
        8             & shape volume \\
        \hline
        9     & shape volume\_density \\
        \hline
        10            & glcm cluster \\
        \hline
        11           & glcm contrast \\
        \hline
        12            & glcm entropy \\
        \hline
        13        & glcm homogeneity \\
        \hline
        14      & glcm joint\_maximum \\
        \hline
    \end{tabular} 
\end{table}


Our dataset and all the source code can be found in the following gihtub
repository. 

Our dataset contains 15 features extracted on 16 different lymphom cancers. All
the features is listed in table. For exact feature definitions see \ref{sec:appendix_feature_definitions}   All
the features is aggregated in a Volume Of Interest (VOI). Each VOI has a
threshold of $0.41 \cdot max$ with respect to the XXX: image series. 
That is, the minimum value in the VOI is larger or equal to 0.41\% of the
maximum value vowel intensity in the tumor. Defining the boundary as a minimum
value is meaningful since the voxel (3D pixel) intensity value in a PET scanner
is positively correlated with cell proliferation. 
All the images is acquired with a PET Scanner XXX:type where different image
reconstruction techniques was applied to each image. In our analysis we will study 3
different reconstruction methods, XXX: names. This will be our classification
targets. Thus, we are interested to see if it is possibly to identify features
that gives good class separation with respect to reconstruction method. This
will be a challenging since our data only contains only 16 samples per
reconstruction. Hence a total of $16 \cdot 3$ data points. 
This will be an exercise in selecting a few good features to prevent
overfitting applied to simple supervised classification models.   




\subsection{Decision Tree Classifier}
A classification decision tree is a predictive model that predicts the target
class of a feature based on a set of binary rules . The decision tree consists of a series of
nodes, branches, and leaves. Each node represents a decision point, each branch
represents a possible outcome, and each leaf represents a final consequence or
result. 

% XXX: explain fig
% and structure in a decision tree
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/descion_tree.png}
    \caption{Decision tree}  
    \label{fig:descision_tree} 
\end{figure}

A typical structure of a decision is shown in figure \ref{fig:descision_tree}. Each rectangle
represents a node in the tree. The build of a decision consists of finding
the feature and feature value (threshold) at each node that maximizes the
purity of each child node resulting from the split. At each node the data is
split into two child nodes based on the threshold value. Values lower than the threshold is
passed to the left child node and values higher is passed to the right child
node. Thus, the tree consists of a set of rules on where to a specific sample. The algorithm is recursive and starts at the
root node (the upper rectangle) on continues until a Leaf node is reached.
That is, one of the squares in the bottom of the figures.     


\subsubsection{The CART algorithm} \hfill

We implemented the CART algorithm to find the optimal splits in the data.  

The CART algorithm splits the data set in two subsets using a single feature
$k$ and a threshold $t_k$ \cite{w44}. The purity of the split is then evaluated
with the cost function: 
\begin{equation*}
    \label{eq:cart} 
    C(k, t_k) = \frac{m_{\text{left}} }{m} G_{\text{left}}+
    \frac{m_{\text{right}} }{m} G_{\text{right}}, 
\end{equation*}

where $G$ is a measure of impurity (see section ),
$m_{\text{left}} $ and $m_{\text{right}} $ is the number of samples in the left
and right subsets resulting from the split. The total number of samples is $m =
m_{left} + m_{right}  $ 

The algorithm iterates through all p features and n samples. For each feature k
and threshod $t_k$, the cost score is evaluated on the two subsets. The
threshold and features that produced the best score is stored in a node. This
process is repeated on the data subsets in a recursive way until a specific
criterion is met. This produces a leaf node in the tree. 

In our implementation the recursion stops if it is not possible to split the
subset further. That is, the number of samples in the subset is equal to one.   
The recursion will also stop if our subset is 100\% pure. That is, the subset
only consist of one class label.  

\subsubsection{Gini factor} \label{sec:gini_factor} \hfill

The Gini factor is used to measure the purity of the data subset and is
calculated as:
\begin{equation*}
    G = 1 - \sum_{i=1}^{c} P_i ^2 
\end{equation*}
where $P_i$ is the probability that a sample belongs to class i. 

For a maximum impurity, where $P_1 = P_2 = 0.5$ the Gini factor is 0.5. A
smaller value indicates a better split. For a perfectly pure class the Gini
factor is 1. That is all the samples belongs to a class k ($P_k = 1$).     


Our class target labels was stored in one hot vector of size $n \times c$, 
where n is the number of samples in the data subset and c is the number of
classes. The probability of each class was calculated with this python snippet:
\begin{lstlisting}[language=Python]
import numpy as np
P = np.sum(y, axis = 0)/y.shape[0]
\end{lstlisting},
where \verb|y| is the one hot vector of our class targets. 

\subsubsection{Feature importance} \label{sec:feature_imporance} 
Our random forest implementation was used to identify most discriminative
features and not used for its predictive power. Again, we will use the Gini
factor to calculate the feature importance. 


We want to quantify the impurity decrease as results of a feature split.
Therefore, we are interested in the change in impurity with respect to a node and its
child nodes. The impurity decrease also needs to take into account the number
of samples. A feature is more important if it is able to separate more samples.       

% We will use the same formula as the scikit learn python library \cite{sklearn}
to estimate the feature importance: 
\begin{equation*}
    \frac{m}{M} \cdot (G  - \frac{m_{\text{left}} }{m} G_{\text{left}}+
    \frac{m_{\text{right}} }{m} G_{\text{right}}), 
\end{equation*}
where $G$, $G_{left} $ and $G_{right} $ as the Gini factor for the parent,
left child and right child node respectively. M is the total number of samples
in our input data. The other quantities are as defined in equation
\ref{eq:cart}. 

All our decision trees has a variable called \verb|feature_importances_|, which
contains the impurity decrease obtained from the best split of a node (except
for leaf nodes).   

When building the tree all features is considered each time a node is to be
split. If the same feature is used multiple times to split nodes in to a tree.
Then, only the split that produces the highest impurity decrease is stored in
the \verb|feature_importances_| variable. Hence, used to evaluate the feature
importance.      



Our decision tree algorithm can be used directly to identify good features.
However, there are some problems. The algorithm usually produces very different
results dependent on small variations in the data. That is, classification
decision trees suffers from high variance. It may also neglect highly
correlated features that produces a slightly poorer separation. To circumvent
these problems we implemented the random forest algorithm. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Random forest}
Random forest is a type of ensemble learning method, which uses multiple
decision trees to make predictions and combines them to create a more accurate
and stable prediction. 

In our first attempt to identify good features (features with good
class separation), we will use a random forest classifier. 
It can be used to identify the features and feature value that splits the data
into the purest classes (see section \ref{sec:feature_imporance})
The algorithm works by constructing multiple decision trees, each of which is trained 
on a subset of the data. 

It uses a technique called bagging, which involves
randomly selecting a subset of data from the training set and building a
decision tree from each subset. 

It also uses feature resampling to reduce the
overfitting of the model. We will implement feature resampling
to better evaluate the performance of all the features. Feature resampling is
done within each node of the descion tree. Hence, the best split at each node is
evaluated on different feature subsets. The general algorithm for a random
forest is listed below. 
\paragraph{Algorithm} 

% Random forest algorithm - cite lecture 44
\fbox{\begin{minipage}{30em}
We will grow of forest of say $\bm{B}$ trees.

For b=1:B

Draw a bootstrap sample from the training data organized in our $\boldsymbol{X}$ matrix.

We grow then a random forest tree $T_b $
based on the bootstrapped data by repeating the steps outlined till we reach
the maximum node size is reached

we select $m \leq p$  variables at random from the p predictors/features

pick the best split point among the $m$ features using for the CART
algorithm and create a new node

split the node into daughter nodes

Output then the ensemble of trees $\{T_b\}^B_1$

and make predictions for either a regression type of problem or a
classification type of problem.
\end{minipage}}

% Discuss differences with own implementation and sklearn

We did compare our own implementation of the random forest to the python
scikit learn function RandomForestClassifier. For sklearn and our own
implementation we used the following parameters, \verb|n\_estimators=100000|,
\verb|max_features='sqrt'|. Where n\_estimators is the number of decision trees
in the forest. A total of $\sqrt{p} $ random features was considered in each
split, where p=15, is the total number of features. Then the models was fitted
to the full dataset. Our fitted classifier will then contains a list of all the
fitted decision trees. The feature\_importances\_ variable obtained from all the
decision trees was used to find the overall best features.  

The results from our own implementation and sklearn did mostly agree in the
feature ranking. However, the impurity decrease obtained with sklearn was
approximately two times larger for all features than obtained with our own
implementation. Our own implementation was also much slower compared to
sklearn. Therefore, the results obtained with the random forest in this algorithm
is obtained with sklearn's implementation. The following python code can easily
be used to reproduce our results:  
\begin{lstlisting}[language=Python]
import numpy as np 
from sklearn.ensemble import RandomForestClassifier
np.random.seed(0)
RandomForestClassifier(n_estimators=10000, max_features='sqrt')
clf.fit(X, y)
importances = np.zeros((clf.n_estimators, X.shape[1]))
for i, tree_clf in enumerate(clf.estimators_): 
    importances[i, :] = tree_clf.feature_importances_
importances = np.mean(importances, axis = 0)
\end{lstlisting}
The mean importance decrease of all the features was stored in the importances
variable.  
% explain elements in code






















% What is one hot vector?
% A one hot vector is a vector of all zeros, except for a single element which is set to one. 
% It is commonly used to represent a categorical variable in machine learning models.


1. Start by selecting the best attribute from the dataset. This attribute will be the root node of the decision 
tree. 

2. Split the data into subsets based on the attribute selected in the first step. 

3. For each subset, select the best attribute and create a decision tree node.

4. Repeat the process for each subset until all the data points have been classified.

5. Finally, test the accuracy of the decision tree by running it on a different set of data.
















\paragraph{Overview} 
\begin{enumerate}
    \item Identify good features by visual inspection 
    \item Feature reduction:
        \begin{itemize}
            \item Manual:
                \begin{itemize}
                    \item Remove correlated features
                \end{itemize}
                
            \item Automatic:
                \begin{itemize}
                    \item Iterate all the features and find best model 
                \end{itemize}
                
        \end{itemize}
        
    \item Evaluate results and find the best ML classification algorithm for dataset 
    
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature section}
\begin{itemize}
    \item not correlated
    \item highly dependent on target variable 
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset and model selection}
\begin{itemize}
    \item Bagging - Bootstrap aggregation
    \item Random forest
\end{itemize}

\begin{itemize}
    \item Sparse data $ \rightarrow $ Focus on simple ML methods
    \item ML classification algorithms suited for sparse data: 
        \begin{itemize}
            \item SVM
        \end{itemize}
        
\end{itemize}



\paragraph{ANOVA} 
Used to select features that is highly dependent on the target variable
\cite{anova}. 











\subsubsection{SVM}








